# Zettelkasten MCP Server Configuration
#
# This file can be placed at:
#   ~/.zettelkasten/.env    (recommended — survives plugin updates)
#   <project-root>/.env     (development use)
#
# Priority: process env (.mcp.json) > project .env > ~/.zettelkasten/.env > defaults

# Directory where notes are stored as markdown files
# Recommend using absolute paths for centralized storage across projects
ZETTELKASTEN_NOTES_DIR=~/.zettelkasten/notes

# SQLite database for indexing and search
ZETTELKASTEN_DATABASE_PATH=~/.zettelkasten/db/zettelkasten.db

# Logging level (DEBUG, INFO, WARNING, ERROR)
ZETTELKASTEN_LOG_LEVEL=INFO

# Optional: Mirror notes to an Obsidian vault
# When set, notes are organized into project-based subdirectories:
#   <vault>/general/Note Title.md
#   <vault>/my-project/Another Note.md
#
# Use zk_sync_to_obsidian tool or ./scripts/sync-to-obsidian to bulk sync
# ZETTELKASTEN_OBSIDIAN_VAULT=/path/to/obsidian/vault/zettelkasten

# Git Versioning (for multi-process concurrency)
# When true, notes are version-controlled with git for conflict detection
# Each note update gets a commit hash that can be used to detect concurrent edits
ZETTELKASTEN_GIT_ENABLED=true

# In-Memory Database (recommended for multi-process environments)
# When true, each process uses its own in-memory SQLite database, eliminating
# database lock contention. The index is rebuilt from markdown files on startup.
# Set to false only if you need to share database state across processes.
ZETTELKASTEN_IN_MEMORY_DB=true

# Server name (used in MCP protocol handshake)
# ZETTELKASTEN_SERVER_NAME=znote-mcp

# =============================================================================
# Semantic Search / Embeddings (optional)
# =============================================================================
# Requires: pip install znote-mcp[semantic]
# When deps are installed, embeddings auto-enable on startup unless explicitly
# disabled here. Set to false to disable even when deps are available.
# ZETTELKASTEN_EMBEDDINGS_ENABLED=false

# Embedding model (HuggingFace model ID)
# ZETTELKASTEN_EMBEDDING_MODEL=Alibaba-NLP/gte-modernbert-base

# Reranker model for improving search result quality
# ZETTELKASTEN_RERANKER_MODEL=Alibaba-NLP/gte-reranker-modernbert-base

# Embedding vector dimension (must match the model's output dimension)
# ZETTELKASTEN_EMBEDDING_DIM=768

# Maximum token length for embedding input (model supports up to 8192)
# Most zettelkasten notes are 200-800 tokens; 2048 covers ~99% of atomic notes.
# Only increase if you have long-form content (literature reviews, structure notes).
# ZETTELKASTEN_EMBEDDING_MAX_TOKENS=2048

# Seconds before idle reranker model is unloaded to free memory (0 = never unload)
# ZETTELKASTEN_RERANKER_IDLE_TIMEOUT=600

# Batch size for embedding operations (reindex, bulk create)
# Higher values are faster but use more memory. The attention mechanism allocates
# memory proportional to batch_size × max_tokens², so both settings interact.
#
# Memory usage guide (approximate peak during reindex):
#   batch=2,  tokens=2048 → ~400MB  (safe for 4GB+ systems)
#   batch=8,  tokens=2048 → ~1.6GB  (safe for 8GB+ systems, DEFAULT)
#   batch=16, tokens=4096 → ~6.4GB  (needs 16GB+ systems)
#   batch=32, tokens=8192 → ~25GB+  (needs 64GB+ systems)
# ZETTELKASTEN_EMBEDDING_BATCH_SIZE=8

# Custom directory for caching downloaded models (default: HuggingFace cache)
# ZETTELKASTEN_EMBEDDING_CACHE_DIR=/path/to/model/cache

# ONNX execution provider preference:
#   "auto"  — detect GPU (CUDA) and fall back to CPU (default)
#   "cpu"   — force CPU only
#   "CUDAExecutionProvider,CPUExecutionProvider" — explicit ordered list
#
# For GPU acceleration, install the semantic-gpu extra instead:
#   pip install znote-mcp[semantic-gpu]
# Note: onnxruntime and onnxruntime-gpu are mutually exclusive packages.
# Do NOT install both [semantic] and [semantic-gpu].
# GPU is only available on x86_64 Linux/Windows with NVIDIA CUDA 12.x.
# ZETTELKASTEN_ONNX_PROVIDERS=auto

# Use INT8 quantized ONNX models for faster inference and smaller memory footprint.
# Quantized models are ~4x smaller (143MB vs 569MB) with ~97% quality retention.
# Requires model_quantized.onnx to exist in the model cache (generate with
# onnxruntime.quantization.quantize_dynamic). Falls back to FP32 if not found.
# ZETTELKASTEN_ONNX_QUANTIZED=false

# Chunk size (in tokens) for splitting long notes before embedding.
# Notes shorter than this are embedded as a single vector.
# Notes longer than this are split into overlapping chunks.
# ZETTELKASTEN_EMBEDDING_CHUNK_SIZE=4096

# Overlap (in tokens) between consecutive chunks.
# Ensures context continuity across chunk boundaries.
# ZETTELKASTEN_EMBEDDING_CHUNK_OVERLAP=256
